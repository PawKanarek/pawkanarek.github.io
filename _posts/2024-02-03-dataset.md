---
layout: post
title: "99 problems"
categories: "post"
---

```
If you having compute problems I feel bad for you son 
I got 99 problems but a speed ain't one
```

![fire_rap][fire_rap]
> "AI-Rapper-Bot write rap-song about Machine Learning on a piece of fire-paper with fire-pencil, Oil on canvas, Beksinski." DALL-E 3

How many problems you can you make before you even start training an AI Model? 

[Last post][last_post] was finished with exciting news about getting control over the Cloud with TPU v3-8 machine, But I must admit that this was  rather is rather storm briniging thundercloud. 

# With great power, comes great complexity

In previous post I also mentioned that the Graphics Designer that I intended to hire was in fact StableDiffusion XL model, and I prepared some very basic, simple scripts that could generate any graphics I could ever imagine, sadly I quickly realized that those scripts aren't compatible with Cloud TPU architecture. But what exactly was wrong?

In todays post I'll explain why powerful machine means big problems.  

Also I hear your feedback, you said that there is too much tech-voodo, so today I will try explain a little bit more.  

Start explaining my problems with with extracting the minimal code that is required to run the SDXL model from the [model card][sdxl] on my local macbook

{% highlight python linenos  %}
from diffusers import DiffusionPipeline
import torch

stable_diffusion = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    variant="fp16",
    use_safetensors=True,
)

stable_diffusion = stable_diffusion.to("mps")

output = stable_diffusion(
    prompt="Gandalf with a shotgun",
    num_inference_steps=50,
)
output.images[0].save("image.png")
{% endhighlight %}

Break this by lines:

<details>
<summary>Click to expand</summary>

 `1` load the [Hugghing Face ðŸ¤— Diffusers][diffusers] library, thanks to this library we can use powerful and complex machine learning models with ease

 `2` loads the [PyTorch][pytorch] - one of most popular library for working with Machine Learning stuff. 

 `4` says we are using `from_pretrained` method on `DiffusionPipeline` class and we are saving results of this method to new variable `stable_diffusion`

 `5` The uniqe name of the model, thanks to this diffusers library knows what to load, obtained from the from the [model card][sdxl]

 `6` specify the floating point precision format in which model will be loaded.  

 `7` specify the floating point precision format in wihch model was saved.

 `8` specify if we want use special called [safetensors][safetensors], creaded mostly to protect for malicous code incjection into model weights. 

 `11` With this line we move all the heavy computation from processor (CPU) to graphics card (GPU). MPS or [Metal Performance Shaders][mps] is library for. 

 `13` run the Stable diffusion with parameters specified in lines 14 and 15, save the ouptut of this process to new variable called `output`

 `14` Specify what prompt we want to get from this model 

 `15` How long the model should craft our image? Anything between 20-60 is good, but i found that with bigger values doesn't improve quality much. 

 `17` get the first image from the ouptut (in programming we start counting usually from the `0`), and save it to the "image.png" file. 

</details>

Now, (assuming that we have installed all nesecery libraries on our machine) we can run this script and after few minuts my macbook generates this  beautiful Gundalf with shotgun and messed up hands.   

![Gandalf][gundalf]
> "Gandalf with a shotgun", Stable Diffusion XL 1.0

Note: There would be less weird artifacts if we would use also the [Refiner][refiner] model in our script. 

# TPU
Great. Now lets use this script on Google TPU v3-8. 



Now we are using [fp16][fp16] and that means that every single weight of a neuron of this model will be loaded to memory as 16bits (2 bytes) number.




 I discovered that to harness the full power of the TPU, I needed to work with Google AutoGrad frameworks [Jax](https://github.com/google/jax) / [Flax](https://github.com/google/flax). There is a nice [Hugging Face blogpost](https://huggingface.co/blog/sdxl_jax) that describes integration of Jax with TPUs. And boy, was this script fast! On my cloud TPU v3-8 device, I could generate 8 images in 24 seconds. In contrast, my MacBook Pro would take approximately 5 minutes to generate a single image with similar model parameters. A quick calculation tell us this Google Cloud TPU is about 100 times faster than my $3k laptop.

For curious minds out there, here are links to my scripts on GitHub for generating images with PyTorch [with_pytorch.py](https://github.com/PawKanarek/spraix/blob/main/generate/with_pytorch.py) and with Flax [with_flax.py](https://github.com/PawKanarek/spraix/blob/main/generate/with_flax.py) 

--- 

## 1. Don't create training dataset from licensed assets


### Dataset? 
When I started this project I knew that to train any neural net model It is required to prepare good training dataset. I've heard that gathering training data is like a 80% of the work. This is relevant when we hear informations about new better-and smaller LLMs, only because they were trained on better train sets, e.g. [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 

So my plan is to fine tune [Stable Diffusion XL](https://stability.ai/stable-diffusion) model in way that one will give me animated sprite in single image that i could reprocess to create a functional sprite animation.
Why not use a base model? Because it will give unexcpected and funny results. And i need consistent sprite format that i could preprocess some basic python script, and as you can see below the base SDXL model retuns inconsistent interference image when i asked for sprite animation of walking dinosaur: 

### Gather internet data

Ideally that dataset should contain pairs of sprite animations with labeled descriptions. Initially my source of sprites was Google Images due to its ease of use, but the quality was often poor. Fortunately, I found the fantastic itch.io platform with ton of content for game dev. 
After an hour or two of gathering free data I had considerably large amounts of animated sprites in various format. Then I made selection of all downloaded content, and I removed everything that wasn't pleasing, based only on my based preference, finally i ended with sprites mostly from itch.io as they had the best quality imho

![[raw_sprite_examples.png]]
Examples of gathered sprites

Take a quick look at above screenshot, on the left we can se single image, that contains animation opening animation of 4 coloured chests in single image, on the right side we that animation sequence is split onto multiple pngs. Thats's ain't uniform data, SDXL wont process this so I have to reprocess that. That's my initial requirements for training dataset
- Each image should be sized 1024x1024 (same size as output from SDXL)
- Single animation per training image 
- Each image should have meaningful description
- Frames of animation should be in equal sizes,
- No distortion artifacts in saved image
- Empty space in image should be minimal


Task: Create dataset
Problem: Every sprite have different size and is saved in different format, data is basicly random
Goal: Save all sprites in unified format and attach labels
### The boring
I started with the easiest one: meaningful description. As title says it was boring and slow process but with that i could start thinking about structure of processing script. During that proces i just had to look closely on every animation to extract information about sizes of each frames, directory/file structure in which they are saved, Iedeally every asset should have that information: Who is on sprite? What is doing? How many frames? In result i created this json structure with all necessary informations
![[json.png]]
json file with sprite description. [source on github](https://github.com/PawKanarek/sprAIx_data_only/blob/114bd4dd2a576f85626180b980f9e911be45cc75/raw_sprites/Medieval%20King%20Pack%202/_sprite_data.json)
### The fun
Now i finally could start with some image processing. The goal is to fit each frame of animation in square of 1024x1024, resize the frames so there is not much empty space left,  and keep the original proportions of frames, so images are not distorted. That was so far the best part of this project, as it was relatively simply, and i could finally write some code in new to me python programming language. As a result i got this nicely looking [make_train_data.py](https://github.com/PawKanarek/sprAIx_data_only/blob/114bd4dd2a576f85626180b980f9e911be45cc75/make_train_data.py) spaghetti script. Finally something bigger than 50 lines of code, my first bigger python script, for sure there is a lot room to improvement, but it does the job, so i will leave it as it is. The dependencies that i used is `JSONWizard` for easier working with jsons, `PIL` for Image manipulation, and finally `black` and `isort` for code style

![[spaghetti.png]]
the spaghetti that i cooked in  [make_train_data.py](https://github.com/PawKanarek/sprAIx_data_only/blob/114bd4dd2a576f85626180b980f9e911be45cc75/make_train_data.py)  script 

### The pain 
... then I just execute python script, and 'viola, the dataset is ready! So lets push it to github repo, make it public available, let the world see how bright it shines. But then i had a some sort of flashbacks that hit me like a train. The information that i was previously intentionally ignoring:
> ![[cannot_redistirbute.png]]
> ![[no_ai.png]]
> ![[attribution_required.png]]
>  'Credit required', 'Licensed Asset', 'NO AI', 'NO REDISTRIBUTION' etc. 

Some of assets that i used to create dataset was licensed by CC BY 04, that means the appreciation credit to author of this assets is required in order to use this asset, and other artists explicitly said they don't want any sort of AI to be used with their work. Some of work was licensed by CC0, that means that credit is appreciated, but no required. And almost all of them said that redistribution in unmodified way is restricted. So i have have to respect the artists will. The pain is that had to track down every asset that i mindlessly downloaded, check its license, and remove everything that i couldn't make public. The second pain is that i saw that information when i was downloading the data. But i wasn't thinking too much about that. Thankfully only small portion of my work was removed in this process, also i ended with creating attribution page for every author that was used in this project, you can check it out [here](https://github.com/PawKanarek/spraix/blob/ab319655e00125dca5361f142920df49c52d4bfd/credits.md)

Lesson for today: don't try to be a pirate. 
![[pirate.jpeg]]
Expressionist oil painting titled 'Pirate' painted with colors of exploding nebulas. Dall-e 3

### Dataset
Final dataset consist file with labeled description `metadata.jsonl` ([JSON Lines](https://jsonlines.org/)) and individual images in `Images` directory. Each description contains information about:
- Number of frames
- Character presented on sprite
- Action character is doing in animation
- what side of world character is facing (N W S E )

![[saved dataset.png]]


### THE HARD

Finally training script completed! No runtime exceptions during training, saving and loading model is flawless. Now we have Problems with itnterference. Nan nan nan.. NANI?!
![[inteference_nani.png]]








![[nanis.jpeg]]
>'Painting by Katsushika Hokusai called 'Nani ?!' depicting an shocked samurAI by the Tensors filed with [nan, nan, nan] ... [nan, nan, nan]' DALL-E 3














I had to learn about this without our helpful Captain Obvious
![[captain_obvious.jpeg]]
Thank you captain obvious, oil on canvas by Meme Master, c. 1664, expressionist style. Dalle-3


[last_post]: {% post_url 2024-01-26-intro %}
[sdxl]: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
[refiner]: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0
[diffusers]: https://huggingface.co/docs/diffusers/index
[pytorch]: https://pytorch.org/
[fp16]: https://en.wikipedia.org/wiki/Half-precision_floating-point_format
[fp32]: https://en.wikipedia.org/wiki/Single-precision_floating-point_format
[safetensors]: https://github.com/huggingface/safetensors
[mps]: https://developer.apple.com/documentation/metalperformanceshaders
[gundalf]: /assets/2024-02-03/gundalf.png
[fire_rap]: /assets/2024-02-03/fire_rap.jpeg
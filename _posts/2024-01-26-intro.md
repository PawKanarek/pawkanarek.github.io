# Human Learning Machine Learning

Greetings, Internet wanderer, and welcome to my not so very first blog post. Join me on my adventure as I train Artificial Intelligence on powerful cloud machines using frameworks that were entirely new to me. Let's start from the beginning, as usual.

TLDR: Story of my life. This post dont inclue any spohisticated technical details. More techo-mumbo-jumbo will come soon.

# The Beginning 

A few months ago, I decided to leave my job after a decade of relentless work as a mobile developer. During my time off, I nearly finished 'Zelda: Tears of the Kingdom' and invested what could only be described as an astronaut's worth of training time in building my factory in 'Factorio'. That was wonderful time and I regret only one thing. In outside world there is AI revolution spinning, and I'm not taking part in it. Then I realized that I should code once again, but this time I wanted to explore new coding areas, since Xamarin wasn't sparking joy anymore.

Since the end of 2022, I have been fascinated by generative AI models, such as ChatGPT and Midjourney, and since I was little kid I have beeen facinated by games. At the begining of my developer career I even made one mobile game! But then quickly my boss noticed that indie games and money don't come along, so we started building proffesional software that could generate nice income. 

Now [I am on my own boat, am steer, fisherman and the fish itself](https://youtu.be/sw9ETYHtzis?si=XltmWlemUTS7iZwF&t=35) and I set the course for combining AI and Game Dev. I planned to 'hire' three robot minions: one for writing code, a second for generating visual content, and a third for making audio effects. Basically, I wanted to use bots to automate game development, just like I automated my factory in Factorio.

When I was picking technologies I choose something within reach of my prior experience, so again, mobile platforms, but this time with Flutter and the [Flame](https://docs.flame-engine.org/) 2D game toolkit. While the game engine is relatively straightforward, I initially struggled to adapt to the aesthetics of the Dart programming language. However, within two afternoons, I had created playable characters, animated movements, and designed working levels. All things considered, having spent more than half decade working with Xamarin, any tool that 'just works' felt amazing! And hey as a self-proclaimed lazy bastard, the goal is to not bother myself with writing too much code.

![[lazy_bastard.png]]
> Behold my 'Lazy Bastard' achievement in Factorio, my virtual diploma in efficient laziness.

Having settled on my tech stack, it was time to assemble my bots team.

To make it happen, I put on my HR suit and started looking around for artificial employees who would be happy to work for the common good. For the developer role, I discovered [Code LLama Instruct 34B](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf), who showed off working Dart code during the recruitment process. Needless to say, it was hired on the spot! 

Next came the Graphic Designer; after some time, I choose  [Stable Diffusion XL base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) with [Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0). I just had to follow the tutorial from the `Model Card` and I could generate any high quality graphics that I could ever imagine.

I was pleased because my workstation (MacBook M1 Pro, 32GB RAM) could handle both new 'employees'. Perhaps they couldn't work simultaneously, but surely they were slow. Speed wasn't my biggest problem, though. I quickly encountered an obstacle that I couldn't easily overcome: my artificial Graphic Designer __wasn't into animations__. Generative models like Stable Diffusion could give me a single image, but generating a sequence of images, like the animation of a walking man or a jumping dinosaur, was beyond its capabilities...

![[3_devs.jpeg]]
>'Old grandpa telling kids story about hiring three Artificial Intelligence models: developer, graphic designer and sound maker. Banksy' DALL-E 3

# The Idea

SDXL can generate whatever I imagine, yet it's limited to a single image. But what if I tell the model to generate a sequence of similarly looking smaller animation frames inside this single image? I could formulate the prompt like this: 

> 'Animated sprite of a jumping dinosaur composed of 6 equal-sized frames' 

![[walking dinosaur.jpeg]]
> This is what we'll get from SDXL. The exact prompt for this image, sadly, has been lost to the sands of time.

This doesn't look good; the model struggles to generate consistent frames for animation. I knew that I could adjust an already trained ML model to better suit my needs, and, after a short talk with Uncle Google, I found a tutorial that guided me through the process of fine-tuning the SDXL model with a `Hugging Face ðŸ¤—` training script called [train_text_to_image_lora_sdxl.py](https://github.com/huggingface/diffusers/blob/457abdf2cf31956a15df7233187b0b358307c7d1/examples/text_to_image/train_text_to_image_lora_sdxl.py)
In theory, fine-tuning is the training of a neural network by slightly adjusting its existing weights to better suit our needs. In reality, I could just run the script and call it a day. However, my laptop quickly objected, as the training wasÂ **very**Â memory hungry - it quickly exceeded my 32GB available RAM. 

![[macbook_weak.png]]
> My MacBook's  32GB of unified memory was quickly exceeded during the training process. That's a byte-sized challenge!

After negotiating with macOS, I managed to 'borrow' some memory from the SSD - or at least that's what I think happened, because the `Activity Monitor` showed that 60GB of memory was in use on my 32GB machine. After a few system crashes, I finally started a training session and discovered that it would take only 189 hours. That's OK with me, but I don't think that my system or my SSD would survive such a hardcore workout.
![[189_h.png]]
> A screenshot of the training output, taken with my phone, as the system was mostly unresponsive. 

# More Brains

A wise article [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) teaches us that with great computing power comes great Machine Learning capability. Happily, the Google [TPU Research Cloud](https://sites.research.google/trc/about/) exists. I applied, and a few days later, they responded with exciting news: for next 30 days I would have access to:
- 5 on-demandÂ TPUÂ v2-8 devices
- 5 on-demandÂ TPUÂ v3-8 devices
- 100 preemptibleÂ TPUÂ v2-8 devices

That's neat, but way too much for my needs!

I focused solely on a single v3-8 machine. Once I attempted to create a v2-8 machine, but I couldn't as Google was throwing `The demand is way too high` errors repeatedly, so I wasn't abusing their kindness anymore. It was also my first time with Google cloud, so I'm very grateful for the detailed documentation, which led me through setting up a new machine, connecting to it via SSH, creating a persistent disk and mounting it on Linux. 
![[htop_v3-8.png]]
> A screenshot showing the `htop` diagnostic view from Google TPU v3-8. 96 CPU cores, with a total of 335 GB of RAM and 8 TPUs with 16GB High bandwidth Memory. Press F to pay respects.

Wow, this is such a powerful machine. I'm starting to feel like a real researcher now, so I've decreased the priority of my creating game-making factory task, as a new quest materialized: 'Make Machine Learning Science'. I want to utilize every watt of this machine to learn, making sure not to waste a single minute. And so, right here, my true journey begins.

### Google TPU vs PyTorch

The first problem I encountered was that my [PyTorch](https://pytorch.org/) scripts for generating images with SDXL aren't compatible with Google TPUs. I discovered that to harness the full power of the TPU, I needed to work with Google AutoGrad frameworks [Jax](https://github.com/google/jax) / [Flax](https://github.com/google/flax). There is a nice [Hugging Face blogpost](https://huggingface.co/blog/sdxl_jax) that describes integration of Jax with TPUs. And boy, was this script fast! On my cloud TPU v3-8 device, I could generate 8 images in 24 seconds. In contrast, my MacBook Pro would take approximately 5 minutes to generate a single image with similar model parameters. A quick calculation tell us this Google Cloud TPU is about 100 times faster than my $3k laptop.
For curious minds out there, here are links to my scripts on GitHub for generating images with PyTorch [with_pytorch.py](https://github.com/PawKanarek/spraix/blob/main/generate/with_pytorch.py) and with Flax [with_flax.py](https://github.com/PawKanarek/spraix/blob/main/generate/with_flax.py)
